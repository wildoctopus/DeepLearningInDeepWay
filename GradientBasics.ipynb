{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GradientBasics.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOY1eBKnlm6gC+6SQZW9zQi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wildoctopus/DeepLearningInDeepWay/blob/main/GradientBasics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04Sk2BcPb1wm"
      },
      "source": [
        "# **Gradient Descent and Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhfQ2AhYfuQJ",
        "outputId": "6db1e344-f36e-4441-e345-f8bec226df89"
      },
      "source": [
        "#Observe output Running without alpha\n",
        "\n",
        "weight = 0.8\n",
        "input = 2\n",
        "goal_pred = 0.9\n",
        "#alpha = 0.1\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "\n",
        "  pred = input*weight\n",
        "  error = (pred - goal_pred)**2              #squared error, to make error always positive. \n",
        "  derivative = (pred - goal_pred)*input\n",
        "  weight = weight - derivative\n",
        "\n",
        "  print(\"Error:\" + str(error) + \" Prediction:\" + str(pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error:0.4900000000000001 Prediction:1.6\n",
            "Error:4.41 Prediction:-1.2000000000000002\n",
            "Error:39.69 Prediction:7.2\n",
            "Error:357.2099999999999 Prediction:-18.0\n",
            "Error:3214.8899999999994 Prediction:57.599999999999994\n",
            "Error:28934.01 Prediction:-169.2\n",
            "Error:260406.09000000003 Prediction:511.2\n",
            "Error:2343654.81 Prediction:-1530.0\n",
            "Error:21092893.290000007 Prediction:4593.6\n",
            "Error:189836039.61000007 Prediction:-13777.200000000003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWAA5EFHOXRU"
      },
      "source": [
        "* **Why error is increasing in every iteration? What can be the reason behind fluctuation of prediction, from very high value to very low value?**\n",
        "\n",
        "* **Why error is computed as a Squared value?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uwwW5J9bym3",
        "outputId": "22571a6f-431b-4b9a-9b54-d4a65d058a26"
      },
      "source": [
        "#Observe output Running with alpha\n",
        "\n",
        "weight = 0.8\n",
        "input = 2\n",
        "goal_pred = 0.9\n",
        "alpha = 0.1\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "\n",
        "  pred = input*weight\n",
        "  error = (pred - goal_pred)**2              #squared error, to make error always positive. \n",
        "  derivative = (pred - goal_pred)*input\n",
        "  weight = weight - derivative*alpha         #uisng gradient descent algorithm to update weight parameter.\n",
        "\n",
        "  print(\"Error:\" + str(error) + \" Prediction:\" + str(pred))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error:0.4900000000000001 Prediction:1.6\n",
            "Error:0.17640000000000003 Prediction:1.32\n",
            "Error:0.06350400000000006 Prediction:1.1520000000000001\n",
            "Error:0.022861440000000035 Prediction:1.0512000000000001\n",
            "Error:0.008230118400000003 Prediction:0.99072\n",
            "Error:0.002962842624000004 Prediction:0.9544320000000001\n",
            "Error:0.00106662334464 Prediction:0.9326592\n",
            "Error:0.00038398440407039913 Prediction:0.91959552\n",
            "Error:0.00013823438546534315 Prediction:0.911757312\n",
            "Error:4.9764378767523225e-05 Prediction:0.9070543872\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugG07OfvRcFu"
      },
      "source": [
        "* **What role does alpha (Learning Rate) plays here?** \n",
        "* **How to decide the value of alpha?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3vyj_gGFsw3"
      },
      "source": [
        "# Mathemetical formula for Gradient Descent -\n",
        "\n",
        "Delta = Error term = (y - y^)f'(h)          \n",
        "\n",
        "f'(h) is the derivative of activation function\n",
        "                                # h = w1x1 + w2x2 + w3x3 + ......\n",
        "                                Weight vector = [w1, w2, w3, w4...]\n",
        "                                Input X = [x1, x2, x3, x4 ....]\n",
        "                                Goal Y = [y1, y2, y3, y4.... ]\n",
        "\n",
        "*** W[i] = W[i] + alpha * delta * X[i]       # weight update uisng Gradient descent.\n",
        "\n",
        "*** Alpha is learning rate. \n",
        "\n"
      ]
    }
  ]
}